{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28193354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ba0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d6cb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drmathew/.local/lib/python3.9/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228811ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a402fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = \"rxfMXxfsdsKGb8urL8shecBbBp37ZBuvU5zo39sm\" # should change this to secret token\n",
    "API_ENDPOINT = f\"https://api.marketaux.com/v1/news/all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf237f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"api_token\": API_TOKEN,\n",
    "    \"entity_types\": \"equity,index,etf\", # should include industries parameter\n",
    "#     \"industries\": \"Technology\",\n",
    "    \"countries\": \"us\",\n",
    "    \"published_after\": \"2024-05-01T13:00\",\n",
    "    \"published_before\": \"2024-05-01T14:00\",\n",
    "    \"domains\": \"finance.yahoo.com\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbda32a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(payload):\n",
    "    \"\"\"\n",
    "    Retrieves URLs of articles from Yahoo Finance\n",
    "    payload: dict, criteria for article selection\n",
    "    return: list, Yahoo Finance article URLs\n",
    "    \"\"\"\n",
    "    r = requests.get(\"https://api.marketaux.com/v1/news/all\", params=payload)\n",
    "    output = r.text\n",
    "    json_output = json.loads(output)\n",
    "    data = json_output['data']\n",
    "    \n",
    "    urls = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        url = data[i]['url']\n",
    "        if url.startswith(\"https://finance.yahoo.com/news\"):\n",
    "            urls.append(url)\n",
    "        \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14925e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_content(url):\n",
    "    \"\"\"\n",
    "    Retrieves article content\n",
    "    url: str, article URL\n",
    "    return: str, article content\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    if r.status_code != 200:\n",
    "        return 'Invalid Response'\n",
    "    \n",
    "    soup = BeautifulSoup(r.content)\n",
    "    body = soup.find('div', attrs={'class': 'caas-body'})\n",
    "    content = \"\"\n",
    "    \n",
    "    for text in body.find_all('p'):\n",
    "        content += text.text\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6bdbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(filename, content):\n",
    "    \"\"\"\n",
    "    Creates and writes article content to file\n",
    "    filename: str, name of file to create\n",
    "    content: str, content to write to file\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c76752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents(start_date, num_days):\n",
    "    \"\"\"\n",
    "    Retrieves article content over a number of days and saves the content to files in the 'articles' folder\n",
    "    start_date: str, date in the format YYYY-MM-DD. articles published on this day are retrieved\n",
    "    num_days: int, the number of days after start_date to retrieve articles for. i.e. if num_days=10, articles published \n",
    "        on the start_date and every day between the start_date and 10 days after are retrieved\n",
    "    \"\"\"\n",
    "    # start_date looks like \"2024-05-01\"\n",
    "    payload = {\n",
    "        \"api_token\": API_TOKEN,\n",
    "        \"entity_types\": \"equity,index,etf\", \n",
    "        \"countries\": \"us\",\n",
    "#         \"published_after\": \"2024-05-10T13:00\", # what time format is supposed to look like, in UTC format \n",
    "#         \"published_before\": \"2024-05-10T14:00\",\n",
    "        \"domains\": \"finance.yahoo.com\"\n",
    "    }\n",
    "    \n",
    "    def get_dates(start_date, num_days):\n",
    "        # Convert the input string to a datetime object\n",
    "        date_obj = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        dates = [date_obj.strftime('%Y-%m-%d')]\n",
    "        # Print the input date\n",
    "        print(date_obj.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "        # Loop to get a total of num_days days\n",
    "        for i in range(1, num_days):\n",
    "            next_day = date_obj + timedelta(days=i)\n",
    "            dates.append(next_day.strftime('%Y-%m-%d'))\n",
    "            print(f\"{next_day.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        return dates\n",
    "        \n",
    "    dates = get_dates(start_date, num_days)\n",
    "    \n",
    "    for date in dates: \n",
    "        for add_hour in range(7): \n",
    "            start = date + \"T\" + str(13 + add_hour) + \":00\" # edit str(xx) for start day\n",
    "            finish = date + \"T\" + str(13 + add_hour + 1) + \":00\"\n",
    "            payload[\"published_after\"] = start\n",
    "            payload[\"published_before\"] = finish\n",
    "            \n",
    "            urls = get_urls(payload)\n",
    "            for i in range(len(urls)):\n",
    "                print(urls[i])\n",
    "                content = get_article_content(urls[i])\n",
    "                if content != \"Invalid Response\": \n",
    "                    filename = os.path.join(\"..\", \"..\", \"new-articles\", \"article_\" + start + \"_\" + str(add_hour) + str(i) + \".txt\")\n",
    "                    write_to_file(filename, content)\n",
    "                else:\n",
    "                    print(\"Above URL Skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64e51541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(selected_model):\n",
    "    \"\"\"\n",
    "    Initializes and stores LLM on memory\n",
    "    selected_model: str, model name to load\n",
    "    return: model variable\n",
    "    \"\"\"\n",
    "    \n",
    "    SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
    "    - Generate human readable output, avoid creating output with gibberish text.\n",
    "    - Generate only the requested output, don't include any other language before or after the requested output.\n",
    "    - Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
    "    - Generate professional language typically used in business documents in North America.\n",
    "    - Never generate offensive or foul language.\n",
    "    \"\"\"\n",
    "\n",
    "    query_wrapper_prompt = PromptTemplate(\n",
    "        \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    "    )\n",
    "\n",
    "    llm = HuggingFaceLLM(\n",
    "        context_window=4096,\n",
    "        max_new_tokens=2048,\n",
    "        generate_kwargs={\"temperature\": 0.1, \"do_sample\": True},\n",
    "        query_wrapper_prompt=query_wrapper_prompt,\n",
    "        tokenizer_name=selected_model,\n",
    "        model_name=selected_model,\n",
    "        device_map=\"auto\",\n",
    "        # change these settings below depending on your GPU\n",
    "        model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True},\n",
    "    )\n",
    "        \n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10e04263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_engine(llm):\n",
    "    \"\"\"\n",
    "    Creates the necessary components to run a RAG pipeline\n",
    "    llm: LLM object loaded into memory\n",
    "    return: SimpleDirectoryReader, VectorStoreIndex, QueryEngine. the first object houses the data, the second object indexes\n",
    "        the data for fast retrieval, and the third object is how we can query the data using the LLM\n",
    "    \"\"\"\n",
    "    set_global_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode\n",
    "    )\n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    \n",
    "    yf_documents = SimpleDirectoryReader(input_dir=\"../../new-articles\").load_data()\n",
    "    yf_index = VectorStoreIndex.from_documents(yf_documents, embed_model=embed_model)\n",
    "    yf_query_engine = yf_index.as_query_engine(llm=llm, streaming=True)\n",
    "    \n",
    "    return yf_documents, yf_index, yf_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41425cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stocks(query_engine, user_prefs):\n",
    "    \"\"\"\n",
    "    Returns set of stocks to invest in based on customer preferences and market news\n",
    "    llm: LLM object loaded into memory\n",
    "    user_prefs: dict, has keys \"num_stocks\", \"risk\", \"industries\", \"return_goals\", contains information about user preferences\n",
    "    return: list, set of stocks matching user preferences returned from LLM based on articles \n",
    "    \"\"\"\n",
    "    \n",
    "    start_prompt = (\"Gather the financial securities that are mentioned across the articles the most in a positive sentiment.\"\n",
    "     f\" The securities gathered can be stocks or ETFs. Return a list of {user_prefs['num_stocks']} securities in which no more than\"\n",
    "     \" half of the securities are from the same industry. \")\n",
    "    risk_averse = (\"This list of securities should be tailored to a very risk averse investor who \"\n",
    "        \"is more focused on stability instead of rapid growth. \")\n",
    "    risk_neutral = \"\"\n",
    "    risk_tolerant = (\"This list of securities should also be tailored to a very risk tolerant investor who \"\n",
    "        \"is more focused on rapid growth instead of stability. This list should include growth stocks with high expected returns \" \n",
    "        \"and the potential for greater risk. \")\n",
    "    industries_prompt = f\"Include multiple stocks with a strong focus on the following industry or industries: {user_prefs['industries']}. \"\n",
    "    end_prompt = (\"Create this list so that they can be put together to make a diversified portfolio. \" \n",
    "        \"List the stock name, stock ticker in parentheses, and the industry associated with it \" \n",
    "        \"and double check that a majority of the industries aren't the same.\")\n",
    "    \n",
    "    prompt_dict = {\n",
    "        \"risk_averse\": risk_averse, \n",
    "        \"risk_neutral\": risk_neutral,\n",
    "        \"risk_tolerant\": risk_tolerant\n",
    "    }\n",
    "    prompt = None\n",
    "    \n",
    "    if user_prefs[\"risk\"] == \"conservative\":\n",
    "        prompt = start_prompt + prompt_dict[\"risk_averse\"]\n",
    "    elif user_prefs[\"risk\"] == \"aggressive\":\n",
    "        prompt = start_prompt + prompt_dict[\"risk_tolerant\"]\n",
    "    else:\n",
    "        prompt = start_prompt + prompt_dict[\"risk_neutral\"]\n",
    "        \n",
    "    if user_prefs['industries']:\n",
    "        prompt += industries_prompt\n",
    "        \n",
    "    prompt += end_prompt\n",
    "        \n",
    "    \n",
    "    print(prompt)\n",
    "    \n",
    "    VALID = False\n",
    "    attempts = 0\n",
    "    \n",
    "    while not VALID:\n",
    "        attempts += 1\n",
    "        print(attempts)\n",
    "        response = query_engine.query(prompt)\n",
    "        response.print_response_stream()\n",
    "    \n",
    "        pattern = r'\\(([A-Za-z]+)\\)'\n",
    "        stocks = re.findall(pattern, response.response)\n",
    "    \n",
    "    \n",
    "        for ticker in stocks:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            if 'regularMarketOpen' not in stock.info:\n",
    "                VALID = False\n",
    "                break\n",
    "            else:\n",
    "                VALID = True\n",
    "                \n",
    "    return stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a348efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_options = [\"conservative\", \"moderate\", \"aggressive\"]\n",
    "industry_options = [\"Energy\", \"Materials\", \"Industrials\", \"Utilities\", \"Healthcare\", \"Financials\", \"Consumer Discretionary\",\n",
    "                    \"Consumer Staples\", \"Technology\", \"Communication Services\", \"Real Estate\"]\n",
    "return_goals = []\n",
    "user_prefs = {\n",
    "    \"num_stocks\": None, \n",
    "    \"risk\": None, \n",
    "    \"industries\": None, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "643d78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA2_7B_CHAT = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "selected_model = LLAMA2_7B_CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aab81ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create_documents(\"2024-05-20\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "775a0e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02cac9becb5424faa7c2d4f0addac1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = get_llm(selected_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30341b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, index, query_engine = get_query_engine(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a38a9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example user preferences\n",
    "user_prefs = {\"num_stocks\": 5, \"risk\": \"conservative\", \"industries\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72a52f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prompt = (\"Gather the financial securities that are mentioned across the articles the most in a positive sentiment.\"\n",
    "     f\" The securities gathered can be stocks or ETFs. Return a list of {user_prefs['num_stocks']} securities in which no more than\"\n",
    "     \" half of the securities are from the same industry. \")\n",
    "risk_averse = (\"This list of securities should be tailored to a very risk averse investor who \"\n",
    "    \"is more focused on stability instead of rapid growth. \")\n",
    "risk_neutral = \"\"\n",
    "risk_tolerant = (\"This list of securities should also be tailored to a very risk tolerant investor who \"\n",
    "    \"is more focused on rapid growth instead of stability. This list should include growth stocks with high expected returns \" \n",
    "    \"and the potential for greater risk. \")\n",
    "industries_prompt = f\"Include multiple stocks with a strong focus on the following industry or industries: {user_prefs['industries']}. \"\n",
    "end_prompt = (\"Create this list so that they can be put together to make a diversified portfolio. \" \n",
    "        \"List the stock name, stock ticker in parentheses, and the industry associated with it \" \n",
    "        \"and double check that a majority of the industries aren't the same.\")\n",
    "\n",
    "prompt_dict = {\n",
    "    \"risk_averse\": risk_averse, \n",
    "    \"risk_neutral\": risk_neutral,\n",
    "    \"risk_tolerant\": risk_tolerant\n",
    "}\n",
    "prompt = None\n",
    "\n",
    "if user_prefs[\"risk\"] == \"conservative\":\n",
    "    prompt = start_prompt + prompt_dict[\"risk_averse\"]\n",
    "elif user_prefs[\"risk\"] == \"aggressive\":\n",
    "    prompt = start_prompt + prompt_dict[\"risk_tolerant\"]\n",
    "else:\n",
    "    prompt = start_prompt + prompt_dict[\"risk_neutral\"]\n",
    "\n",
    "if user_prefs['industries']:\n",
    "    prompt += industries_prompt\n",
    "\n",
    "prompt += end_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c6e361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 15:18:49.729099: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "r = llm.complete(\"Write a Python function that only prints even numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "405d7452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! Here is a Python function that only prints even numbers:\n",
      "\n",
      "def print_even_numbers():\n",
      "    for number in range(10):\n",
      "        if number % 2 == 0:\n",
      "            print(number)\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d7b9cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, I would suggest the following three stocks to consider for investment:\n",
      "\n",
      "1. Nvidia (NVDA): As a leader in the "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2777/339511060.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tell me 3 stocks to buy.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_response_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/core/base/response/schema.py\u001b[0m in \u001b[0;36mprint_response_stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_txt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_gen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mresponse_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mresponse_txt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/core/llms/llm.py\u001b[0m in \u001b[0;36mgen\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTokenGen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompletion_response_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/core/llms/callbacks.py\u001b[0m in \u001b[0;36mwrapped_gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCompletionResponseGen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                         \u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf_return_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                             dispatcher.event(\n\u001b[1;32m    338\u001b[0m                                 LLMCompletionEndEvent(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/llama_index/llms/huggingface/base.py\u001b[0m in \u001b[0;36mgen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCompletionResponseGen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mCompletionResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/generation/streamers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_signal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r = query_engine.query(\"Tell me 3 stocks to buy.\")\n",
    "r.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7810776",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = query_engine.query(prompt)\n",
    "r.print_response_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
